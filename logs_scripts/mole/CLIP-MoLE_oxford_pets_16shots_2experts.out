created virtual environment CPython3.10.13.final.0-64 in 36750ms
  creator CPython3Posix(dest=/localscratch/pedro36.57174889.0/env, clear=False, no_vcs_ignore=False, global=False)
  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/home/pedro36/.local/share/virtualenv)
    added seed packages: pip==25.1.1, setuptools==80.7.1, wheel==0.45.1+computecanada
  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v4, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already satisfied: pip in /localscratch/pedro36.57174889.0/env/lib/python3.10/site-packages (25.1.1)
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v4, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3/torch-2.6.0+computecanada-cp310-cp310-linux_x86_64.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/generic/torchvision-0.21.0+computecanada-cp310-cp310-linux_x86_64.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3/torchaudio-2.6.0+computecanada-cp310-cp310-linux_x86_64.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/ftfy-6.3.1+computecanada-py3-none-any.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/generic/scipy-1.15.1+computecanada-cp310-cp310-linux_x86_64.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/regex-2024.9.11+computecanada-cp310-cp310-linux_x86_64.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/tqdm-4.67.1+computecanada-py3-none-any.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/gdown-5.2.0+computecanada-py3-none-any.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3/pandas-2.2.3+computecanada-cp310-cp310-linux_x86_64.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/filelock-3.18.0+computecanada-py3-none-any.whl (from torch)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/typing_extensions-4.14.0+computecanada-py3-none-any.whl (from torch)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/networkx-3.5+computecanada-py3-none-any.whl (from torch)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/jinja2-3.1.6+computecanada-py3-none-any.whl (from torch)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/fsspec-2025.5.1+computecanada-py3-none-any.whl (from torch)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/sympy-1.13.1+computecanada-py3-none-any.whl (from torch)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/mpmath-1.3.0+computecanada-py3-none-any.whl (from sympy==1.13.1->torch)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/generic/numpy-2.2.2+computecanada-cp310-cp310-linux_x86_64.whl (from torchvision)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3/Pillow_SIMD-9.5.0.post2+computecanada-cp310-cp310-linux_x86_64.whl (from torchvision)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/wcwidth-0.2.13+computecanada-py2.py3-none-any.whl (from ftfy)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/beautifulsoup4-4.13.4+computecanada-py3-none-any.whl (from gdown)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/requests-2.32.4+computecanada-py3-none-any.whl (from gdown)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/python_dateutil-2.9.0.post0+computecanada-py2.py3-none-any.whl (from pandas)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/pytz-2025.2+computecanada-py2.py3-none-any.whl (from pandas)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/tzdata-2025.2+computecanada-py2.py3-none-any.whl (from pandas)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/six-1.17.0+computecanada-py2.py3-none-any.whl (from python-dateutil>=2.8.2->pandas)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/soupsieve-2.7+computecanada-py3-none-any.whl (from beautifulsoup4->gdown)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/MarkupSafe-2.1.5+computecanada-cp310-cp310-linux_x86_64.whl (from jinja2->torch)
INFO: pip is looking at multiple versions of networkx to determine which version is compatible with other requirements. This could take a while.
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/networkx-3.4.2+computecanada-py3-none-any.whl (from torch)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/charset_normalizer-3.4.2+computecanada-py3-none-any.whl (from requests[socks]->gdown)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/idna-3.10+computecanada-py3-none-any.whl (from requests[socks]->gdown)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/urllib3-2.5.0+computecanada-py3-none-any.whl (from requests[socks]->gdown)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/certifi-2025.6.15+computecanada-py3-none-any.whl (from requests[socks]->gdown)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/PySocks-1.7.1+computecanada-py3-none-any.whl (from requests[socks]->gdown)
Installing collected packages: wcwidth, pytz, mpmath, urllib3, tzdata, typing-extensions, tqdm, sympy, soupsieve, six, regex, PySocks, pillow-simd, numpy, networkx, MarkupSafe, idna, ftfy, fsspec, filelock, charset-normalizer, certifi, scipy, requests, python-dateutil, jinja2, beautifulsoup4, torch, pandas, torchvision, torchaudio, gdown

Successfully installed MarkupSafe-2.1.5+computecanada PySocks-1.7.1+computecanada beautifulsoup4-4.13.4+computecanada certifi-2025.6.15+computecanada charset-normalizer-3.4.2+computecanada filelock-3.18.0+computecanada fsspec-2025.5.1+computecanada ftfy-6.3.1+computecanada gdown-5.2.0+computecanada idna-3.10+computecanada jinja2-3.1.6+computecanada mpmath-1.3.0+computecanada networkx-3.4.2+computecanada numpy-2.2.2+computecanada pandas-2.2.3+computecanada pillow-simd-9.5.0.post2+computecanada python-dateutil-2.9.0.post0+computecanada pytz-2025.2+computecanada regex-2024.9.11+computecanada requests-2.32.4+computecanada scipy-1.15.1+computecanada six-1.17.0+computecanada soupsieve-2.7+computecanada sympy-1.13.1+computecanada torch-2.6.0+computecanada torchaudio-2.6.0+computecanada torchvision-0.21.0+computecanada tqdm-4.67.1+computecanada typing-extensions-4.14.0+computecanada tzdata-2025.2+computecanada urllib3-2.5.0+computecanada wcwidth-0.2.13+computecanada
Preparing dataset.
Reading split from /home/pedro36/projects/def-leszek/pedro36/datasets/DATA/OxfordPets/split_zhou_OxfordPets.json
Creating a 4-shot dataset
Creating a 16-shot dataset

Getting textual features as CLIP's classifier.

Loading visual features and labels from val set.

Loading visual features and labels from test set.

**** Zero-shot CLIP's test accuracy: 88.20. ****

Residual Attention Block 0: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 1: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 2: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 3: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 4: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 5: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 6: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 7: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 8: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 9: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 10: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 11: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 0: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 1: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 2: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 3: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 4: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 5: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 6: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 7: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 8: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 9: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 10: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 11: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Trainable parameters: ['visual.transformer.resblocks.0.attn.q_proj.router.gate.weight', 'visual.transformer.resblocks.0.attn.q_proj.experts.0.w_lora_A', 'visual.transformer.resblocks.0.attn.q_proj.experts.0.w_lora_B', 'visual.transformer.resblocks.0.attn.q_proj.experts.1.w_lora_A', 'visual.transformer.resblocks.0.attn.q_proj.experts.1.w_lora_B', 'visual.transformer.resblocks.0.attn.k_proj.router.gate.weight', 'visual.transformer.resblocks.0.attn.k_proj.experts.0.w_lora_A', 'visual.transformer.resblocks.0.attn.k_proj.experts.0.w_lora_B', 'visual.transformer.resblocks.0.attn.k_proj.experts.1.w_lora_A', 'visual.transformer.resblocks.0.attn.k_proj.experts.1.w_lora_B', 'visual.transformer.resblocks.0.attn.v_proj.router.gate.weight', 'visual.transformer.resblocks.0.attn.v_proj.experts.0.w_lora_A', 'visual.transformer.resblocks.0.attn.v_proj.experts.0.w_lora_B', 'visual.transformer.resblocks.0.attn.v_proj.experts.1.w_lora_A', 'visual.transformer.resblocks.0.attn.v_proj.experts.1.w_lora_B', 'visual.transformer.resblocks.1.attn.q_proj.router.gate.weight', 'visual.transformer.resblocks.1.attn.q_proj.experts.0.w_lora_A', 'visual.transformer.resblocks.1.attn.q_proj.experts.0.w_lora_B', 'visual.transformer.resblocks.1.attn.q_proj.experts.1.w_lora_A', 'visual.transformer.resblocks.1.attn.q_proj.experts.1.w_lora_B', 'visual.transformer.resblocks.1.attn.k_proj.router.gate.weight', 'visual.transformer.resblocks.1.attn.k_proj.experts.0.w_lora_A', 'visual.transformer.resblocks.1.attn.k_proj.experts.0.w_lora_B', 'visual.transformer.resblocks.1.attn.k_proj.experts.1.w_lora_A', 'visual.transformer.resblocks.1.attn.k_proj.experts.1.w_lora_B', 'visual.transformer.resblocks.1.attn.v_proj.router.gate.weight', 'visual.transformer.resblocks.1.attn.v_proj.experts.0.w_lora_A', 'visual.transformer.resblocks.1.attn.v_proj.experts.0.w_lora_B', 'visual.transformer.resblocks.1.attn.v_proj.experts.1.w_lora_A', 'visual.transformer.resblocks.1.attn.v_proj.experts.1.w_lora_B', 'visual.transformer.resblocks.2.attn.q_proj.router.gate.weight', 'visual.transformer.resblocks.2.attn.q_proj.experts.0.w_lora_A', 'visual.transformer.resblocks.2.attn.q_proj.experts.0.w_lora_B', 'visual.transformer.resblocks.2.attn.q_proj.experts.1.w_lora_A', 'visual.transformer.resblocks.2.attn.q_proj.experts.1.w_lora_B', 'visual.transformer.resblocks.2.attn.k_proj.router.gate.weight', 'visual.transformer.resblocks.2.attn.k_proj.experts.0.w_lora_A', 'visual.transformer.resblocks.2.attn.k_proj.experts.0.w_lora_B', 'visual.transformer.resblocks.2.attn.k_proj.experts.1.w_lora_A', 'visual.transformer.resblocks.2.attn.k_proj.experts.1.w_lora_B', 'visual.transformer.resblocks.2.attn.v_proj.router.gate.weight', 'visual.transformer.resblocks.2.attn.v_proj.experts.0.w_lora_A', 'visual.transformer.resblocks.2.attn.v_proj.experts.0.w_lora_B', 'visual.transformer.resblocks.2.attn.v_proj.experts.1.w_lora_A', 'visual.transformer.resblocks.2.attn.v_proj.experts.1.w_lora_B', 'visual.transformer.resblocks.3.attn.q_proj.router.gate.weight', 'visual.transformer.resblocks.3.attn.q_proj.experts.0.w_lora_A', 'visual.transformer.resblocks.3.attn.q_proj.experts.0.w_lora_B', 'visual.transformer.resblocks.3.attn.q_proj.experts.1.w_lora_A', 'visual.transformer.resblocks.3.attn.q_proj.experts.1.w_lora_B', 'visual.transformer.resblocks.3.attn.k_proj.router.gate.weight', 'visual.transformer.resblocks.3.attn.k_proj.experts.0.w_lora_A', 'visual.transformer.resblocks.3.attn.k_proj.experts.0.w_lora_B', 'visual.transformer.resblocks.3.attn.k_proj.experts.1.w_lora_A', 'visual.transformer.resblocks.3.attn.k_proj.experts.1.w_lora_B', 'visual.transformer.resblocks.3.attn.v_proj.router.gate.weight', 'visual.transformer.resblocks.3.attn.v_proj.experts.0.w_lora_A', 'visual.transformer.resblocks.3.attn.v_proj.experts.0.w_lora_B', 'visual.transformer.resblocks.3.attn.v_proj.experts.1.w_lora_A', 'visual.transformer.resblocks.3.attn.v_proj.experts.1.w_lora_B', 'visual.transformer.resblocks.4.attn.q_proj.router.gate.weight', 'visual.transformer.resblocks.4.attn.q_proj.experts.0.w_lora_A', 'visual.transformer.resblocks.4.attn.q_proj.experts.0.w_lora_B', 'visual.transformer.resblocks.4.attn.q_proj.experts.1.w_lora_A', 'visual.transformer.resblocks.4.attn.q_proj.experts.1.w_lora_B', 'visual.transformer.resblocks.4.attn.k_proj.router.gate.weight', 'visual.transformer.resblocks.4.attn.k_proj.experts.0.w_lora_A', 'visual.transformer.resblocks.4.attn.k_proj.experts.0.w_lora_B', 'visual.transformer.resblocks.4.attn.k_proj.experts.1.w_lora_A', 'visual.transformer.resblocks.4.attn.k_proj.experts.1.w_lora_B', 'visual.transformer.resblocks.4.attn.v_proj.router.gate.weight', 'visual.transformer.resblocks.4.attn.v_proj.experts.0.w_lora_A', 'visual.transformer.resblocks.4.attn.v_proj.experts.0.w_lora_B', 'visual.transformer.resblocks.4.attn.v_proj.experts.1.w_lora_A', 'visual.transformer.resblocks.4.attn.v_proj.experts.1.w_lora_B', 'visual.transformer.resblocks.5.attn.q_proj.router.gate.weight', 'visual.transformer.resblocks.5.attn.q_proj.experts.0.w_lora_A', 'visual.transformer.resblocks.5.attn.q_proj.experts.0.w_lora_B', 'visual.transformer.resblocks.5.attn.q_proj.experts.1.w_lora_A', 'visual.transformer.resblocks.5.attn.q_proj.experts.1.w_lora_B', 'visual.transformer.resblocks.5.attn.k_proj.router.gate.weight', 'visual.transformer.resblocks.5.attn.k_proj.experts.0.w_lora_A', 'visual.transformer.resblocks.5.attn.k_proj.experts.0.w_lora_B', 'visual.transformer.resblocks.5.attn.k_proj.experts.1.w_lora_A', 'visual.transformer.resblocks.5.attn.k_proj.experts.1.w_lora_B', 'visual.transformer.resblocks.5.attn.v_proj.router.gate.weight', 'visual.transformer.resblocks.5.attn.v_proj.experts.0.w_lora_A', 'visual.transformer.resblocks.5.attn.v_proj.experts.0.w_lora_B', 'visual.transformer.resblocks.5.attn.v_proj.experts.1.w_lora_A', 'visual.transformer.resblocks.5.attn.v_proj.experts.1.w_lora_B', 'visual.transformer.resblocks.6.attn.q_proj.router.gate.weight', 'visual.transformer.resblocks.6.attn.q_proj.experts.0.w_lora_A', 'visual.transformer.resblocks.6.attn.q_proj.experts.0.w_lora_B', 'visual.transformer.resblocks.6.attn.q_proj.experts.1.w_lora_A', 'visual.transformer.resblocks.6.attn.q_proj.experts.1.w_lora_B', 'visual.transformer.resblocks.6.attn.k_proj.router.gate.weight', 'visual.transformer.resblocks.6.attn.k_proj.experts.0.w_lora_A', 'visual.transformer.resblocks.6.attn.k_proj.experts.0.w_lora_B', 'visual.transformer.resblocks.6.attn.k_proj.experts.1.w_lora_A', 'visual.transformer.resblocks.6.attn.k_proj.experts.1.w_lora_B', 'visual.transformer.resblocks.6.attn.v_proj.router.gate.weight', 'visual.transformer.resblocks.6.attn.v_proj.experts.0.w_lora_A', 'visual.transformer.resblocks.6.attn.v_proj.experts.0.w_lora_B', 'visual.transformer.resblocks.6.attn.v_proj.experts.1.w_lora_A', 'visual.transformer.resblocks.6.attn.v_proj.experts.1.w_lora_B', 'visual.transformer.resblocks.7.attn.q_proj.router.gate.weight', 'visual.transformer.resblocks.7.attn.q_proj.experts.0.w_lora_A', 'visual.transformer.resblocks.7.attn.q_proj.experts.0.w_lora_B', 'visual.transformer.resblocks.7.attn.q_proj.experts.1.w_lora_A', 'visual.transformer.resblocks.7.attn.q_proj.experts.1.w_lora_B', 'visual.transformer.resblocks.7.attn.k_proj.router.gate.weight', 'visual.transformer.resblocks.7.attn.k_proj.experts.0.w_lora_A', 'visual.transformer.resblocks.7.attn.k_proj.experts.0.w_lora_B', 'visual.transformer.resblocks.7.attn.k_proj.experts.1.w_lora_A', 'visual.transformer.resblocks.7.attn.k_proj.experts.1.w_lora_B', 'visual.transformer.resblocks.7.attn.v_proj.router.gate.weight', 'visual.transformer.resblocks.7.attn.v_proj.experts.0.w_lora_A', 'visual.transformer.resblocks.7.attn.v_proj.experts.0.w_lora_B', 'visual.transformer.resblocks.7.attn.v_proj.experts.1.w_lora_A', 'visual.transformer.resblocks.7.attn.v_proj.experts.1.w_lora_B', 'visual.transformer.resblocks.8.attn.q_proj.router.gate.weight', 'visual.transformer.resblocks.8.attn.q_proj.experts.0.w_lora_A', 'visual.transformer.resblocks.8.attn.q_proj.experts.0.w_lora_B', 'visual.transformer.resblocks.8.attn.q_proj.experts.1.w_lora_A', 'visual.transformer.resblocks.8.attn.q_proj.experts.1.w_lora_B', 'visual.transformer.resblocks.8.attn.k_proj.router.gate.weight', 'visual.transformer.resblocks.8.attn.k_proj.experts.0.w_lora_A', 'visual.transformer.resblocks.8.attn.k_proj.experts.0.w_lora_B', 'visual.transformer.resblocks.8.attn.k_proj.experts.1.w_lora_A', 'visual.transformer.resblocks.8.attn.k_proj.experts.1.w_lora_B', 'visual.transformer.resblocks.8.attn.v_proj.router.gate.weight', 'visual.transformer.resblocks.8.attn.v_proj.experts.0.w_lora_A', 'visual.transformer.resblocks.8.attn.v_proj.experts.0.w_lora_B', 'visual.transformer.resblocks.8.attn.v_proj.experts.1.w_lora_A', 'visual.transformer.resblocks.8.attn.v_proj.experts.1.w_lora_B', 'visual.transformer.resblocks.9.attn.q_proj.router.gate.weight', 'visual.transformer.resblocks.9.attn.q_proj.experts.0.w_lora_A', 'visual.transformer.resblocks.9.attn.q_proj.experts.0.w_lora_B', 'visual.transformer.resblocks.9.attn.q_proj.experts.1.w_lora_A', 'visual.transformer.resblocks.9.attn.q_proj.experts.1.w_lora_B', 'visual.transformer.resblocks.9.attn.k_proj.router.gate.weight', 'visual.transformer.resblocks.9.attn.k_proj.experts.0.w_lora_A', 'visual.transformer.resblocks.9.attn.k_proj.experts.0.w_lora_B', 'visual.transformer.resblocks.9.attn.k_proj.experts.1.w_lora_A', 'visual.transformer.resblocks.9.attn.k_proj.experts.1.w_lora_B', 'visual.transformer.resblocks.9.attn.v_proj.router.gate.weight', 'visual.transformer.resblocks.9.attn.v_proj.experts.0.w_lora_A', 'visual.transformer.resblocks.9.attn.v_proj.experts.0.w_lora_B', 'visual.transformer.resblocks.9.attn.v_proj.experts.1.w_lora_A', 'visual.transformer.resblocks.9.attn.v_proj.experts.1.w_lora_B', 'visual.transformer.resblocks.10.attn.q_proj.router.gate.weight', 'visual.transformer.resblocks.10.attn.q_proj.experts.0.w_lora_A', 'visual.transformer.resblocks.10.attn.q_proj.experts.0.w_lora_B', 'visual.transformer.resblocks.10.attn.q_proj.experts.1.w_lora_A', 'visual.transformer.resblocks.10.attn.q_proj.experts.1.w_lora_B', 'visual.transformer.resblocks.10.attn.k_proj.router.gate.weight', 'visual.transformer.resblocks.10.attn.k_proj.experts.0.w_lora_A', 'visual.transformer.resblocks.10.attn.k_proj.experts.0.w_lora_B', 'visual.transformer.resblocks.10.attn.k_proj.experts.1.w_lora_A', 'visual.transformer.resblocks.10.attn.k_proj.experts.1.w_lora_B', 'visual.transformer.resblocks.10.attn.v_proj.router.gate.weight', 'visual.transformer.resblocks.10.attn.v_proj.experts.0.w_lora_A', 'visual.transformer.resblocks.10.attn.v_proj.experts.0.w_lora_B', 'visual.transformer.resblocks.10.attn.v_proj.experts.1.w_lora_A', 'visual.transformer.resblocks.10.attn.v_proj.experts.1.w_lora_B', 'visual.transformer.resblocks.11.attn.q_proj.router.gate.weight', 'visual.transformer.resblocks.11.attn.q_proj.experts.0.w_lora_A', 'visual.transformer.resblocks.11.attn.q_proj.experts.0.w_lora_B', 'visual.transformer.resblocks.11.attn.q_proj.experts.1.w_lora_A', 'visual.transformer.resblocks.11.attn.q_proj.experts.1.w_lora_B', 'visual.transformer.resblocks.11.attn.k_proj.router.gate.weight', 'visual.transformer.resblocks.11.attn.k_proj.experts.0.w_lora_A', 'visual.transformer.resblocks.11.attn.k_proj.experts.0.w_lora_B', 'visual.transformer.resblocks.11.attn.k_proj.experts.1.w_lora_A', 'visual.transformer.resblocks.11.attn.k_proj.experts.1.w_lora_B', 'visual.transformer.resblocks.11.attn.v_proj.router.gate.weight', 'visual.transformer.resblocks.11.attn.v_proj.experts.0.w_lora_A', 'visual.transformer.resblocks.11.attn.v_proj.experts.0.w_lora_B', 'visual.transformer.resblocks.11.attn.v_proj.experts.1.w_lora_A', 'visual.transformer.resblocks.11.attn.v_proj.experts.1.w_lora_B', 'transformer.resblocks.0.attn.q_proj.router.gate.weight', 'transformer.resblocks.0.attn.q_proj.experts.0.w_lora_A', 'transformer.resblocks.0.attn.q_proj.experts.0.w_lora_B', 'transformer.resblocks.0.attn.q_proj.experts.1.w_lora_A', 'transformer.resblocks.0.attn.q_proj.experts.1.w_lora_B', 'transformer.resblocks.0.attn.k_proj.router.gate.weight', 'transformer.resblocks.0.attn.k_proj.experts.0.w_lora_A', 'transformer.resblocks.0.attn.k_proj.experts.0.w_lora_B', 'transformer.resblocks.0.attn.k_proj.experts.1.w_lora_A', 'transformer.resblocks.0.attn.k_proj.experts.1.w_lora_B', 'transformer.resblocks.0.attn.v_proj.router.gate.weight', 'transformer.resblocks.0.attn.v_proj.experts.0.w_lora_A', 'transformer.resblocks.0.attn.v_proj.experts.0.w_lora_B', 'transformer.resblocks.0.attn.v_proj.experts.1.w_lora_A', 'transformer.resblocks.0.attn.v_proj.experts.1.w_lora_B', 'transformer.resblocks.1.attn.q_proj.router.gate.weight', 'transformer.resblocks.1.attn.q_proj.experts.0.w_lora_A', 'transformer.resblocks.1.attn.q_proj.experts.0.w_lora_B', 'transformer.resblocks.1.attn.q_proj.experts.1.w_lora_A', 'transformer.resblocks.1.attn.q_proj.experts.1.w_lora_B', 'transformer.resblocks.1.attn.k_proj.router.gate.weight', 'transformer.resblocks.1.attn.k_proj.experts.0.w_lora_A', 'transformer.resblocks.1.attn.k_proj.experts.0.w_lora_B', 'transformer.resblocks.1.attn.k_proj.experts.1.w_lora_A', 'transformer.resblocks.1.attn.k_proj.experts.1.w_lora_B', 'transformer.resblocks.1.attn.v_proj.router.gate.weight', 'transformer.resblocks.1.attn.v_proj.experts.0.w_lora_A', 'transformer.resblocks.1.attn.v_proj.experts.0.w_lora_B', 'transformer.resblocks.1.attn.v_proj.experts.1.w_lora_A', 'transformer.resblocks.1.attn.v_proj.experts.1.w_lora_B', 'transformer.resblocks.2.attn.q_proj.router.gate.weight', 'transformer.resblocks.2.attn.q_proj.experts.0.w_lora_A', 'transformer.resblocks.2.attn.q_proj.experts.0.w_lora_B', 'transformer.resblocks.2.attn.q_proj.experts.1.w_lora_A', 'transformer.resblocks.2.attn.q_proj.experts.1.w_lora_B', 'transformer.resblocks.2.attn.k_proj.router.gate.weight', 'transformer.resblocks.2.attn.k_proj.experts.0.w_lora_A', 'transformer.resblocks.2.attn.k_proj.experts.0.w_lora_B', 'transformer.resblocks.2.attn.k_proj.experts.1.w_lora_A', 'transformer.resblocks.2.attn.k_proj.experts.1.w_lora_B', 'transformer.resblocks.2.attn.v_proj.router.gate.weight', 'transformer.resblocks.2.attn.v_proj.experts.0.w_lora_A', 'transformer.resblocks.2.attn.v_proj.experts.0.w_lora_B', 'transformer.resblocks.2.attn.v_proj.experts.1.w_lora_A', 'transformer.resblocks.2.attn.v_proj.experts.1.w_lora_B', 'transformer.resblocks.3.attn.q_proj.router.gate.weight', 'transformer.resblocks.3.attn.q_proj.experts.0.w_lora_A', 'transformer.resblocks.3.attn.q_proj.experts.0.w_lora_B', 'transformer.resblocks.3.attn.q_proj.experts.1.w_lora_A', 'transformer.resblocks.3.attn.q_proj.experts.1.w_lora_B', 'transformer.resblocks.3.attn.k_proj.router.gate.weight', 'transformer.resblocks.3.attn.k_proj.experts.0.w_lora_A', 'transformer.resblocks.3.attn.k_proj.experts.0.w_lora_B', 'transformer.resblocks.3.attn.k_proj.experts.1.w_lora_A', 'transformer.resblocks.3.attn.k_proj.experts.1.w_lora_B', 'transformer.resblocks.3.attn.v_proj.router.gate.weight', 'transformer.resblocks.3.attn.v_proj.experts.0.w_lora_A', 'transformer.resblocks.3.attn.v_proj.experts.0.w_lora_B', 'transformer.resblocks.3.attn.v_proj.experts.1.w_lora_A', 'transformer.resblocks.3.attn.v_proj.experts.1.w_lora_B', 'transformer.resblocks.4.attn.q_proj.router.gate.weight', 'transformer.resblocks.4.attn.q_proj.experts.0.w_lora_A', 'transformer.resblocks.4.attn.q_proj.experts.0.w_lora_B', 'transformer.resblocks.4.attn.q_proj.experts.1.w_lora_A', 'transformer.resblocks.4.attn.q_proj.experts.1.w_lora_B', 'transformer.resblocks.4.attn.k_proj.router.gate.weight', 'transformer.resblocks.4.attn.k_proj.experts.0.w_lora_A', 'transformer.resblocks.4.attn.k_proj.experts.0.w_lora_B', 'transformer.resblocks.4.attn.k_proj.experts.1.w_lora_A', 'transformer.resblocks.4.attn.k_proj.experts.1.w_lora_B', 'transformer.resblocks.4.attn.v_proj.router.gate.weight', 'transformer.resblocks.4.attn.v_proj.experts.0.w_lora_A', 'transformer.resblocks.4.attn.v_proj.experts.0.w_lora_B', 'transformer.resblocks.4.attn.v_proj.experts.1.w_lora_A', 'transformer.resblocks.4.attn.v_proj.experts.1.w_lora_B', 'transformer.resblocks.5.attn.q_proj.router.gate.weight', 'transformer.resblocks.5.attn.q_proj.experts.0.w_lora_A', 'transformer.resblocks.5.attn.q_proj.experts.0.w_lora_B', 'transformer.resblocks.5.attn.q_proj.experts.1.w_lora_A', 'transformer.resblocks.5.attn.q_proj.experts.1.w_lora_B', 'transformer.resblocks.5.attn.k_proj.router.gate.weight', 'transformer.resblocks.5.attn.k_proj.experts.0.w_lora_A', 'transformer.resblocks.5.attn.k_proj.experts.0.w_lora_B', 'transformer.resblocks.5.attn.k_proj.experts.1.w_lora_A', 'transformer.resblocks.5.attn.k_proj.experts.1.w_lora_B', 'transformer.resblocks.5.attn.v_proj.router.gate.weight', 'transformer.resblocks.5.attn.v_proj.experts.0.w_lora_A', 'transformer.resblocks.5.attn.v_proj.experts.0.w_lora_B', 'transformer.resblocks.5.attn.v_proj.experts.1.w_lora_A', 'transformer.resblocks.5.attn.v_proj.experts.1.w_lora_B', 'transformer.resblocks.6.attn.q_proj.router.gate.weight', 'transformer.resblocks.6.attn.q_proj.experts.0.w_lora_A', 'transformer.resblocks.6.attn.q_proj.experts.0.w_lora_B', 'transformer.resblocks.6.attn.q_proj.experts.1.w_lora_A', 'transformer.resblocks.6.attn.q_proj.experts.1.w_lora_B', 'transformer.resblocks.6.attn.k_proj.router.gate.weight', 'transformer.resblocks.6.attn.k_proj.experts.0.w_lora_A', 'transformer.resblocks.6.attn.k_proj.experts.0.w_lora_B', 'transformer.resblocks.6.attn.k_proj.experts.1.w_lora_A', 'transformer.resblocks.6.attn.k_proj.experts.1.w_lora_B', 'transformer.resblocks.6.attn.v_proj.router.gate.weight', 'transformer.resblocks.6.attn.v_proj.experts.0.w_lora_A', 'transformer.resblocks.6.attn.v_proj.experts.0.w_lora_B', 'transformer.resblocks.6.attn.v_proj.experts.1.w_lora_A', 'transformer.resblocks.6.attn.v_proj.experts.1.w_lora_B', 'transformer.resblocks.7.attn.q_proj.router.gate.weight', 'transformer.resblocks.7.attn.q_proj.experts.0.w_lora_A', 'transformer.resblocks.7.attn.q_proj.experts.0.w_lora_B', 'transformer.resblocks.7.attn.q_proj.experts.1.w_lora_A', 'transformer.resblocks.7.attn.q_proj.experts.1.w_lora_B', 'transformer.resblocks.7.attn.k_proj.router.gate.weight', 'transformer.resblocks.7.attn.k_proj.experts.0.w_lora_A', 'transformer.resblocks.7.attn.k_proj.experts.0.w_lora_B', 'transformer.resblocks.7.attn.k_proj.experts.1.w_lora_A', 'transformer.resblocks.7.attn.k_proj.experts.1.w_lora_B', 'transformer.resblocks.7.attn.v_proj.router.gate.weight', 'transformer.resblocks.7.attn.v_proj.experts.0.w_lora_A', 'transformer.resblocks.7.attn.v_proj.experts.0.w_lora_B', 'transformer.resblocks.7.attn.v_proj.experts.1.w_lora_A', 'transformer.resblocks.7.attn.v_proj.experts.1.w_lora_B', 'transformer.resblocks.8.attn.q_proj.router.gate.weight', 'transformer.resblocks.8.attn.q_proj.experts.0.w_lora_A', 'transformer.resblocks.8.attn.q_proj.experts.0.w_lora_B', 'transformer.resblocks.8.attn.q_proj.experts.1.w_lora_A', 'transformer.resblocks.8.attn.q_proj.experts.1.w_lora_B', 'transformer.resblocks.8.attn.k_proj.router.gate.weight', 'transformer.resblocks.8.attn.k_proj.experts.0.w_lora_A', 'transformer.resblocks.8.attn.k_proj.experts.0.w_lora_B', 'transformer.resblocks.8.attn.k_proj.experts.1.w_lora_A', 'transformer.resblocks.8.attn.k_proj.experts.1.w_lora_B', 'transformer.resblocks.8.attn.v_proj.router.gate.weight', 'transformer.resblocks.8.attn.v_proj.experts.0.w_lora_A', 'transformer.resblocks.8.attn.v_proj.experts.0.w_lora_B', 'transformer.resblocks.8.attn.v_proj.experts.1.w_lora_A', 'transformer.resblocks.8.attn.v_proj.experts.1.w_lora_B', 'transformer.resblocks.9.attn.q_proj.router.gate.weight', 'transformer.resblocks.9.attn.q_proj.experts.0.w_lora_A', 'transformer.resblocks.9.attn.q_proj.experts.0.w_lora_B', 'transformer.resblocks.9.attn.q_proj.experts.1.w_lora_A', 'transformer.resblocks.9.attn.q_proj.experts.1.w_lora_B', 'transformer.resblocks.9.attn.k_proj.router.gate.weight', 'transformer.resblocks.9.attn.k_proj.experts.0.w_lora_A', 'transformer.resblocks.9.attn.k_proj.experts.0.w_lora_B', 'transformer.resblocks.9.attn.k_proj.experts.1.w_lora_A', 'transformer.resblocks.9.attn.k_proj.experts.1.w_lora_B', 'transformer.resblocks.9.attn.v_proj.router.gate.weight', 'transformer.resblocks.9.attn.v_proj.experts.0.w_lora_A', 'transformer.resblocks.9.attn.v_proj.experts.0.w_lora_B', 'transformer.resblocks.9.attn.v_proj.experts.1.w_lora_A', 'transformer.resblocks.9.attn.v_proj.experts.1.w_lora_B', 'transformer.resblocks.10.attn.q_proj.router.gate.weight', 'transformer.resblocks.10.attn.q_proj.experts.0.w_lora_A', 'transformer.resblocks.10.attn.q_proj.experts.0.w_lora_B', 'transformer.resblocks.10.attn.q_proj.experts.1.w_lora_A', 'transformer.resblocks.10.attn.q_proj.experts.1.w_lora_B', 'transformer.resblocks.10.attn.k_proj.router.gate.weight', 'transformer.resblocks.10.attn.k_proj.experts.0.w_lora_A', 'transformer.resblocks.10.attn.k_proj.experts.0.w_lora_B', 'transformer.resblocks.10.attn.k_proj.experts.1.w_lora_A', 'transformer.resblocks.10.attn.k_proj.experts.1.w_lora_B', 'transformer.resblocks.10.attn.v_proj.router.gate.weight', 'transformer.resblocks.10.attn.v_proj.experts.0.w_lora_A', 'transformer.resblocks.10.attn.v_proj.experts.0.w_lora_B', 'transformer.resblocks.10.attn.v_proj.experts.1.w_lora_A', 'transformer.resblocks.10.attn.v_proj.experts.1.w_lora_B', 'transformer.resblocks.11.attn.q_proj.router.gate.weight', 'transformer.resblocks.11.attn.q_proj.experts.0.w_lora_A', 'transformer.resblocks.11.attn.q_proj.experts.0.w_lora_B', 'transformer.resblocks.11.attn.q_proj.experts.1.w_lora_A', 'transformer.resblocks.11.attn.q_proj.experts.1.w_lora_B', 'transformer.resblocks.11.attn.k_proj.router.gate.weight', 'transformer.resblocks.11.attn.k_proj.experts.0.w_lora_A', 'transformer.resblocks.11.attn.k_proj.experts.0.w_lora_B', 'transformer.resblocks.11.attn.k_proj.experts.1.w_lora_A', 'transformer.resblocks.11.attn.k_proj.experts.1.w_lora_B', 'transformer.resblocks.11.attn.v_proj.router.gate.weight', 'transformer.resblocks.11.attn.v_proj.experts.0.w_lora_A', 'transformer.resblocks.11.attn.v_proj.experts.0.w_lora_B', 'transformer.resblocks.11.attn.v_proj.experts.1.w_lora_A', 'transformer.resblocks.11.attn.v_proj.experts.1.w_lora_B']
LR: 0.000200, Acc: 74.4932, Loss: 0.9452
LR: 0.000200, Acc: 75.8446, Loss: 0.8363
LR: 0.000200, Acc: 77.3649, Loss: 0.6882
LR: 0.000200, Acc: 82.0946, Loss: 0.6114
LR: 0.000200, Acc: 82.2635, Loss: 0.5808
LR: 0.000200, Acc: 80.4054, Loss: 0.6247
LR: 0.000200, Acc: 84.7973, Loss: 0.4972
LR: 0.000200, Acc: 82.0946, Loss: 0.5541
LR: 0.000200, Acc: 82.6014, Loss: 0.5014
LR: 0.000200, Acc: 84.2905, Loss: 0.5121
LR: 0.000200, Acc: 85.8108, Loss: 0.4471
LR: 0.000200, Acc: 86.6554, Loss: 0.4397
LR: 0.000200, Acc: 84.4595, Loss: 0.4857
LR: 0.000199, Acc: 86.4865, Loss: 0.4291
LR: 0.000199, Acc: 86.6554, Loss: 0.4434
LR: 0.000199, Acc: 88.1757, Loss: 0.3734
LR: 0.000199, Acc: 89.5270, Loss: 0.3636
LR: 0.000199, Acc: 88.6824, Loss: 0.3994
LR: 0.000199, Acc: 88.6824, Loss: 0.3754
LR: 0.000199, Acc: 87.5000, Loss: 0.4017
LR: 0.000199, Acc: 86.9932, Loss: 0.4255
LR: 0.000199, Acc: 88.5135, Loss: 0.3323
LR: 0.000199, Acc: 88.8514, Loss: 0.3470
LR: 0.000198, Acc: 90.0338, Loss: 0.3451
LR: 0.000198, Acc: 87.3311, Loss: 0.3629
LR: 0.000198, Acc: 88.3446, Loss: 0.3880
LR: 0.000198, Acc: 89.3581, Loss: 0.3535
LR: 0.000198, Acc: 89.6959, Loss: 0.3166
LR: 0.000198, Acc: 89.8649, Loss: 0.2992
LR: 0.000198, Acc: 92.0608, Loss: 0.2810
LR: 0.000197, Acc: 90.0338, Loss: 0.3477
LR: 0.000197, Acc: 89.8649, Loss: 0.3307
LR: 0.000197, Acc: 90.8784, Loss: 0.2793
LR: 0.000197, Acc: 90.3716, Loss: 0.2910
LR: 0.000197, Acc: 89.6959, Loss: 0.3246
LR: 0.000196, Acc: 90.5405, Loss: 0.2999
LR: 0.000196, Acc: 89.8649, Loss: 0.3002
LR: 0.000196, Acc: 91.0473, Loss: 0.3352
LR: 0.000196, Acc: 91.8919, Loss: 0.3034
LR: 0.000196, Acc: 92.2297, Loss: 0.2621
LR: 0.000195, Acc: 93.9189, Loss: 0.2139
LR: 0.000195, Acc: 93.0743, Loss: 0.2355
LR: 0.000195, Acc: 91.8919, Loss: 0.2712
LR: 0.000195, Acc: 90.2027, Loss: 0.3361
LR: 0.000194, Acc: 89.6959, Loss: 0.3239
LR: 0.000194, Acc: 94.5946, Loss: 0.1756
LR: 0.000194, Acc: 93.4122, Loss: 0.2183
LR: 0.000194, Acc: 93.4122, Loss: 0.2351
LR: 0.000193, Acc: 90.7095, Loss: 0.3266
LR: 0.000193, Acc: 92.5676, Loss: 0.2078
LR: 0.000193, Acc: 94.0878, Loss: 0.2061
LR: 0.000193, Acc: 93.9189, Loss: 0.2257
LR: 0.000192, Acc: 94.0878, Loss: 0.2359
LR: 0.000192, Acc: 92.9054, Loss: 0.2609
LR: 0.000192, Acc: 92.3986, Loss: 0.2555
LR: 0.000191, Acc: 91.2162, Loss: 0.2780
LR: 0.000191, Acc: 94.2568, Loss: 0.2026
LR: 0.000191, Acc: 91.8919, Loss: 0.2452
LR: 0.000191, Acc: 93.2432, Loss: 0.2283
LR: 0.000190, Acc: 92.5676, Loss: 0.2808
LR: 0.000190, Acc: 93.9189, Loss: 0.2096
LR: 0.000190, Acc: 93.2432, Loss: 0.2276
LR: 0.000189, Acc: 93.5811, Loss: 0.2166
LR: 0.000189, Acc: 93.9189, Loss: 0.2220
LR: 0.000189, Acc: 93.9189, Loss: 0.1997
LR: 0.000188, Acc: 92.2297, Loss: 0.2673
LR: 0.000188, Acc: 93.2432, Loss: 0.2318
LR: 0.000187, Acc: 94.5946, Loss: 0.1893
LR: 0.000187, Acc: 93.0743, Loss: 0.2391
LR: 0.000187, Acc: 94.2568, Loss: 0.2274
LR: 0.000186, Acc: 93.5811, Loss: 0.2228
LR: 0.000186, Acc: 94.2568, Loss: 0.2058
LR: 0.000186, Acc: 94.2568, Loss: 0.1822
LR: 0.000185, Acc: 92.9054, Loss: 0.2399
LR: 0.000185, Acc: 93.0743, Loss: 0.2477
LR: 0.000184, Acc: 94.7635, Loss: 0.2159
LR: 0.000184, Acc: 93.2432, Loss: 0.2097
LR: 0.000184, Acc: 94.9324, Loss: 0.1873
LR: 0.000183, Acc: 93.9189, Loss: 0.1842
LR: 0.000183, Acc: 94.9324, Loss: 0.1810
LR: 0.000182, Acc: 94.0878, Loss: 0.1819
LR: 0.000182, Acc: 92.0608, Loss: 0.2524
LR: 0.000182, Acc: 93.7500, Loss: 0.1912
LR: 0.000181, Acc: 95.1014, Loss: 0.1888
LR: 0.000181, Acc: 95.2703, Loss: 0.1396
LR: 0.000180, Acc: 93.5811, Loss: 0.2110
LR: 0.000180, Acc: 92.3986, Loss: 0.2318
LR: 0.000179, Acc: 93.7500, Loss: 0.2501
LR: 0.000179, Acc: 92.0608, Loss: 0.2411
LR: 0.000178, Acc: 95.1014, Loss: 0.1833
LR: 0.000178, Acc: 94.2568, Loss: 0.1892
LR: 0.000177, Acc: 94.5946, Loss: 0.1847
LR: 0.000177, Acc: 93.5811, Loss: 0.2140
LR: 0.000177, Acc: 93.5811, Loss: 0.2046
LR: 0.000176, Acc: 94.5946, Loss: 0.1864
LR: 0.000176, Acc: 93.5811, Loss: 0.2090
LR: 0.000175, Acc: 93.9189, Loss: 0.1800
LR: 0.000175, Acc: 95.1014, Loss: 0.1812
LR: 0.000174, Acc: 94.9324, Loss: 0.1826
LR: 0.000174, Acc: 94.4257, Loss: 0.1706
LR: 0.000173, Acc: 95.1014, Loss: 0.1753
LR: 0.000173, Acc: 95.2703, Loss: 0.1395
LR: 0.000172, Acc: 92.3986, Loss: 0.2016
LR: 0.000172, Acc: 95.4392, Loss: 0.1691
LR: 0.000171, Acc: 95.6081, Loss: 0.1704
LR: 0.000170, Acc: 94.0878, Loss: 0.1766
LR: 0.000170, Acc: 95.1014, Loss: 0.1653
LR: 0.000169, Acc: 95.4392, Loss: 0.1763
LR: 0.000169, Acc: 95.9459, Loss: 0.1201
LR: 0.000168, Acc: 94.5946, Loss: 0.1911
LR: 0.000168, Acc: 94.7635, Loss: 0.1792
LR: 0.000167, Acc: 94.4257, Loss: 0.1731
LR: 0.000167, Acc: 95.1014, Loss: 0.1570
LR: 0.000166, Acc: 96.1149, Loss: 0.1232
LR: 0.000166, Acc: 94.2568, Loss: 0.1936
LR: 0.000165, Acc: 95.2703, Loss: 0.1748
LR: 0.000164, Acc: 95.7770, Loss: 0.1359
LR: 0.000164, Acc: 94.9324, Loss: 0.1589
LR: 0.000163, Acc: 96.1149, Loss: 0.1768
LR: 0.000163, Acc: 93.0743, Loss: 0.2006
LR: 0.000162, Acc: 95.2703, Loss: 0.1758
LR: 0.000162, Acc: 94.9324, Loss: 0.1544
LR: 0.000161, Acc: 94.4257, Loss: 0.1862
LR: 0.000160, Acc: 94.5946, Loss: 0.2299
LR: 0.000160, Acc: 94.4257, Loss: 0.2040
LR: 0.000159, Acc: 95.7770, Loss: 0.1527
LR: 0.000159, Acc: 94.0878, Loss: 0.2113
LR: 0.000158, Acc: 94.9324, Loss: 0.1624
LR: 0.000157, Acc: 96.4527, Loss: 0.1290
LR: 0.000157, Acc: 96.2838, Loss: 0.1377
LR: 0.000156, Acc: 94.9324, Loss: 0.1779
LR: 0.000156, Acc: 95.7770, Loss: 0.1402
LR: 0.000155, Acc: 94.9324, Loss: 0.1701
LR: 0.000154, Acc: 95.6081, Loss: 0.1414
LR: 0.000154, Acc: 95.9459, Loss: 0.1501
LR: 0.000153, Acc: 95.4392, Loss: 0.1639
LR: 0.000152, Acc: 94.9324, Loss: 0.1566
LR: 0.000152, Acc: 94.9324, Loss: 0.1609
LR: 0.000151, Acc: 95.1014, Loss: 0.2038
LR: 0.000150, Acc: 94.0878, Loss: 0.1920
LR: 0.000150, Acc: 94.7635, Loss: 0.1820
LR: 0.000149, Acc: 94.7635, Loss: 0.1573
LR: 0.000149, Acc: 95.6081, Loss: 0.1604
LR: 0.000148, Acc: 96.2838, Loss: 0.1166
LR: 0.000147, Acc: 93.0743, Loss: 0.2271
LR: 0.000147, Acc: 94.0878, Loss: 0.2098
LR: 0.000146, Acc: 95.7770, Loss: 0.1370
LR: 0.000145, Acc: 95.7770, Loss: 0.1268
LR: 0.000145, Acc: 94.5946, Loss: 0.1754
LR: 0.000144, Acc: 95.1014, Loss: 0.1678
LR: 0.000143, Acc: 92.5676, Loss: 0.2277
LR: 0.000143, Acc: 94.4257, Loss: 0.1683
LR: 0.000142, Acc: 95.2703, Loss: 0.1519
LR: 0.000141, Acc: 97.8041, Loss: 0.0906
LR: 0.000141, Acc: 94.2568, Loss: 0.1665
LR: 0.000140, Acc: 96.4527, Loss: 0.1221
LR: 0.000139, Acc: 95.6081, Loss: 0.1338
LR: 0.000139, Acc: 97.6351, Loss: 0.1025
LR: 0.000138, Acc: 95.2703, Loss: 0.1697
LR: 0.000137, Acc: 94.7635, Loss: 0.1611
LR: 0.000136, Acc: 95.6081, Loss: 0.1648
LR: 0.000136, Acc: 95.2703, Loss: 0.1493
LR: 0.000135, Acc: 95.2703, Loss: 0.1483
LR: 0.000134, Acc: 96.1149, Loss: 0.1529
LR: 0.000134, Acc: 95.6081, Loss: 0.1366
LR: 0.000133, Acc: 95.6081, Loss: 0.1324
LR: 0.000132, Acc: 95.1014, Loss: 0.1303
LR: 0.000132, Acc: 96.1149, Loss: 0.1293
LR: 0.000131, Acc: 95.7770, Loss: 0.1416
LR: 0.000130, Acc: 95.1014, Loss: 0.1493
LR: 0.000129, Acc: 95.4392, Loss: 0.1278
LR: 0.000129, Acc: 94.9324, Loss: 0.1508
LR: 0.000128, Acc: 94.9324, Loss: 0.1803
LR: 0.000127, Acc: 96.4527, Loss: 0.1213
LR: 0.000127, Acc: 97.2973, Loss: 0.0868
LR: 0.000126, Acc: 96.6216, Loss: 0.1225
LR: 0.000125, Acc: 97.4662, Loss: 0.1180
LR: 0.000124, Acc: 96.4527, Loss: 0.1339
LR: 0.000124, Acc: 93.7500, Loss: 0.1540
LR: 0.000123, Acc: 97.2973, Loss: 0.0905
LR: 0.000122, Acc: 96.1149, Loss: 0.1216
LR: 0.000122, Acc: 95.2703, Loss: 0.1395
LR: 0.000121, Acc: 95.4392, Loss: 0.1412
LR: 0.000120, Acc: 97.1284, Loss: 0.1114
LR: 0.000119, Acc: 96.7905, Loss: 0.1120
LR: 0.000119, Acc: 96.1149, Loss: 0.1380
LR: 0.000118, Acc: 95.6081, Loss: 0.1513
LR: 0.000117, Acc: 96.2838, Loss: 0.1317
LR: 0.000116, Acc: 95.9459, Loss: 0.1122
LR: 0.000116, Acc: 95.1014, Loss: 0.1801
LR: 0.000115, Acc: 96.2838, Loss: 0.1532
LR: 0.000114, Acc: 96.7905, Loss: 0.1241
LR: 0.000113, Acc: 96.6216, Loss: 0.1067
LR: 0.000113, Acc: 97.6351, Loss: 0.1120
LR: 0.000112, Acc: 97.6351, Loss: 0.0804
LR: 0.000111, Acc: 97.4662, Loss: 0.0945
LR: 0.000111, Acc: 96.4527, Loss: 0.1013
LR: 0.000110, Acc: 96.9595, Loss: 0.1076
LR: 0.000109, Acc: 96.9595, Loss: 0.1217
LR: 0.000108, Acc: 95.9459, Loss: 0.1270
LR: 0.000108, Acc: 94.9324, Loss: 0.1562
LR: 0.000107, Acc: 96.2838, Loss: 0.1274
LR: 0.000106, Acc: 96.7905, Loss: 0.1168
LR: 0.000105, Acc: 96.7905, Loss: 0.1164
LR: 0.000105, Acc: 95.9459, Loss: 0.1344
LR: 0.000104, Acc: 96.2838, Loss: 0.1374
LR: 0.000103, Acc: 96.4527, Loss: 0.1054
LR: 0.000102, Acc: 96.7905, Loss: 0.1079
LR: 0.000102, Acc: 94.7635, Loss: 0.1741
LR: 0.000101, Acc: 96.2838, Loss: 0.1330
LR: 0.000100, Acc: 95.4392, Loss: 0.1534
LR: 0.000099, Acc: 95.7770, Loss: 0.1540
LR: 0.000099, Acc: 96.9595, Loss: 0.0929
LR: 0.000098, Acc: 96.2838, Loss: 0.1243
LR: 0.000097, Acc: 96.2838, Loss: 0.1220
LR: 0.000096, Acc: 97.2973, Loss: 0.0916
LR: 0.000096, Acc: 96.6216, Loss: 0.1187
LR: 0.000095, Acc: 96.9595, Loss: 0.1126
LR: 0.000094, Acc: 96.9595, Loss: 0.0923
LR: 0.000093, Acc: 96.1149, Loss: 0.1344
LR: 0.000093, Acc: 95.7770, Loss: 0.1202
LR: 0.000092, Acc: 95.2703, Loss: 0.1553
LR: 0.000091, Acc: 96.9595, Loss: 0.1007
LR: 0.000091, Acc: 96.6216, Loss: 0.1317
LR: 0.000090, Acc: 97.4662, Loss: 0.0918
LR: 0.000089, Acc: 95.6081, Loss: 0.1747
LR: 0.000088, Acc: 96.4527, Loss: 0.1293
LR: 0.000088, Acc: 96.4527, Loss: 0.1197
LR: 0.000087, Acc: 96.7905, Loss: 0.1118
LR: 0.000086, Acc: 97.8041, Loss: 0.0659
LR: 0.000085, Acc: 96.2838, Loss: 0.1258
LR: 0.000085, Acc: 95.9459, Loss: 0.1288
LR: 0.000084, Acc: 96.1149, Loss: 0.1416
LR: 0.000083, Acc: 95.6081, Loss: 0.1459
LR: 0.000082, Acc: 95.9459, Loss: 0.1326
LR: 0.000082, Acc: 95.4392, Loss: 0.1320
LR: 0.000081, Acc: 95.6081, Loss: 0.1134
LR: 0.000080, Acc: 97.2973, Loss: 0.1122
LR: 0.000080, Acc: 96.7905, Loss: 0.1137
LR: 0.000079, Acc: 95.4392, Loss: 0.1601
LR: 0.000078, Acc: 97.8041, Loss: 0.0856
LR: 0.000077, Acc: 96.9595, Loss: 0.0939
LR: 0.000077, Acc: 96.7905, Loss: 0.1053
LR: 0.000076, Acc: 96.4527, Loss: 0.1026
LR: 0.000075, Acc: 96.7905, Loss: 0.1200
LR: 0.000074, Acc: 97.1284, Loss: 0.0966
LR: 0.000074, Acc: 96.7905, Loss: 0.1139
LR: 0.000073, Acc: 94.9324, Loss: 0.1509
LR: 0.000072, Acc: 95.9459, Loss: 0.1408
LR: 0.000072, Acc: 97.8041, Loss: 0.0887
LR: 0.000071, Acc: 97.2973, Loss: 0.1168
LR: 0.000070, Acc: 96.2838, Loss: 0.1255
LR: 0.000069, Acc: 96.7905, Loss: 0.1045
LR: 0.000069, Acc: 96.1149, Loss: 0.1528
LR: 0.000068, Acc: 95.7770, Loss: 0.1228
LR: 0.000067, Acc: 95.4392, Loss: 0.1308
LR: 0.000067, Acc: 96.7905, Loss: 0.1067
LR: 0.000066, Acc: 98.4797, Loss: 0.0730
LR: 0.000065, Acc: 96.7905, Loss: 0.1032
LR: 0.000065, Acc: 96.9595, Loss: 0.0996
LR: 0.000064, Acc: 96.7905, Loss: 0.1111
LR: 0.000063, Acc: 96.6216, Loss: 0.1157
LR: 0.000063, Acc: 95.9459, Loss: 0.1195
LR: 0.000062, Acc: 97.2973, Loss: 0.1114
LR: 0.000061, Acc: 96.9595, Loss: 0.1088
LR: 0.000060, Acc: 96.1149, Loss: 0.1501
LR: 0.000060, Acc: 96.7905, Loss: 0.1198
LR: 0.000059, Acc: 97.2973, Loss: 0.0778
LR: 0.000058, Acc: 95.1014, Loss: 0.1315
LR: 0.000058, Acc: 96.9595, Loss: 0.1003
LR: 0.000057, Acc: 95.9459, Loss: 0.1201
LR: 0.000056, Acc: 96.7905, Loss: 0.1253
LR: 0.000056, Acc: 96.9595, Loss: 0.1003
LR: 0.000055, Acc: 96.9595, Loss: 0.0966
LR: 0.000054, Acc: 97.6351, Loss: 0.0971
LR: 0.000054, Acc: 97.2973, Loss: 0.1088
LR: 0.000053, Acc: 95.7770, Loss: 0.1161
LR: 0.000052, Acc: 97.1284, Loss: 0.1122
LR: 0.000052, Acc: 97.1284, Loss: 0.0807
LR: 0.000051, Acc: 96.9595, Loss: 0.1103
LR: 0.000051, Acc: 97.1284, Loss: 0.0999
LR: 0.000050, Acc: 96.4527, Loss: 0.1061
LR: 0.000049, Acc: 95.1014, Loss: 0.1466
LR: 0.000049, Acc: 96.7905, Loss: 0.1124
LR: 0.000048, Acc: 97.6351, Loss: 0.0624
LR: 0.000047, Acc: 96.6216, Loss: 0.1118
LR: 0.000047, Acc: 95.6081, Loss: 0.1178
LR: 0.000046, Acc: 97.1284, Loss: 0.1002
LR: 0.000046, Acc: 96.7905, Loss: 0.1128
LR: 0.000045, Acc: 96.9595, Loss: 0.1035
LR: 0.000044, Acc: 96.4527, Loss: 0.1113
LR: 0.000044, Acc: 96.6216, Loss: 0.1155
LR: 0.000043, Acc: 96.4527, Loss: 0.1297
LR: 0.000042, Acc: 97.1284, Loss: 0.0967
LR: 0.000042, Acc: 96.2838, Loss: 0.1120
LR: 0.000041, Acc: 96.7905, Loss: 0.1273
LR: 0.000041, Acc: 97.4662, Loss: 0.0795
LR: 0.000040, Acc: 97.2973, Loss: 0.0918
LR: 0.000039, Acc: 97.9730, Loss: 0.0744
LR: 0.000039, Acc: 97.6351, Loss: 0.0844
LR: 0.000038, Acc: 96.6216, Loss: 0.1125
LR: 0.000038, Acc: 97.8041, Loss: 0.0896
LR: 0.000037, Acc: 97.2973, Loss: 0.1015
LR: 0.000037, Acc: 96.9595, Loss: 0.1068
LR: 0.000036, Acc: 96.4527, Loss: 0.1023
LR: 0.000035, Acc: 96.7905, Loss: 0.1173
LR: 0.000035, Acc: 97.2973, Loss: 0.0948
LR: 0.000034, Acc: 96.6216, Loss: 0.1166
LR: 0.000034, Acc: 96.7905, Loss: 0.1200
LR: 0.000033, Acc: 97.6351, Loss: 0.0806
LR: 0.000033, Acc: 96.6216, Loss: 0.1141
LR: 0.000032, Acc: 95.9459, Loss: 0.1096
LR: 0.000032, Acc: 97.1284, Loss: 0.1189
LR: 0.000031, Acc: 96.9595, Loss: 0.1085
LR: 0.000031, Acc: 97.6351, Loss: 0.0915
LR: 0.000030, Acc: 96.7905, Loss: 0.0988
LR: 0.000030, Acc: 97.6351, Loss: 0.0753
LR: 0.000029, Acc: 96.6216, Loss: 0.1017
LR: 0.000028, Acc: 97.4662, Loss: 0.0906
LR: 0.000028, Acc: 97.4662, Loss: 0.1083
LR: 0.000027, Acc: 96.7905, Loss: 0.0950
LR: 0.000027, Acc: 97.4662, Loss: 0.0935
LR: 0.000026, Acc: 96.9595, Loss: 0.1017
LR: 0.000026, Acc: 97.6351, Loss: 0.0772
LR: 0.000025, Acc: 96.4527, Loss: 0.1152
LR: 0.000025, Acc: 96.9595, Loss: 0.1019
LR: 0.000025, Acc: 97.1284, Loss: 0.0914
LR: 0.000024, Acc: 97.6351, Loss: 0.0881
LR: 0.000024, Acc: 97.6351, Loss: 0.0754
LR: 0.000023, Acc: 97.9730, Loss: 0.0751
LR: 0.000023, Acc: 96.9595, Loss: 0.1167
LR: 0.000022, Acc: 97.4662, Loss: 0.0936
LR: 0.000022, Acc: 97.4662, Loss: 0.0979
LR: 0.000021, Acc: 98.1419, Loss: 0.0799
LR: 0.000021, Acc: 97.2973, Loss: 0.1003
LR: 0.000020, Acc: 95.9459, Loss: 0.1137
LR: 0.000020, Acc: 97.4662, Loss: 0.0925
LR: 0.000020, Acc: 97.8041, Loss: 0.0811
LR: 0.000019, Acc: 96.6216, Loss: 0.1038
LR: 0.000019, Acc: 98.8176, Loss: 0.0507
LR: 0.000018, Acc: 96.6216, Loss: 0.1126
LR: 0.000018, Acc: 96.4527, Loss: 0.1113
LR: 0.000017, Acc: 97.4662, Loss: 0.1092
LR: 0.000017, Acc: 97.2973, Loss: 0.0815
LR: 0.000017, Acc: 96.9595, Loss: 0.1058
LR: 0.000016, Acc: 97.6351, Loss: 0.0913
LR: 0.000016, Acc: 95.1014, Loss: 0.1852
LR: 0.000015, Acc: 97.6351, Loss: 0.0946
LR: 0.000015, Acc: 96.9595, Loss: 0.1005
LR: 0.000015, Acc: 97.1284, Loss: 0.1085
LR: 0.000014, Acc: 97.2973, Loss: 0.0946
LR: 0.000014, Acc: 97.6351, Loss: 0.0946
LR: 0.000014, Acc: 97.9730, Loss: 0.0661
LR: 0.000013, Acc: 97.2973, Loss: 0.1084
LR: 0.000013, Acc: 99.1554, Loss: 0.0544
LR: 0.000012, Acc: 95.2703, Loss: 0.1403
LR: 0.000012, Acc: 96.2838, Loss: 0.1225
LR: 0.000012, Acc: 96.6216, Loss: 0.0811
LR: 0.000011, Acc: 96.2838, Loss: 0.0980
LR: 0.000011, Acc: 97.1284, Loss: 0.0958
LR: 0.000011, Acc: 97.2973, Loss: 0.0967
LR: 0.000011, Acc: 97.8041, Loss: 0.0843
LR: 0.000010, Acc: 96.4527, Loss: 0.1238
LR: 0.000010, Acc: 96.2838, Loss: 0.1378
LR: 0.000010, Acc: 96.7905, Loss: 0.1028
LR: 0.000009, Acc: 97.6351, Loss: 0.0964
LR: 0.000009, Acc: 97.4662, Loss: 0.0877
LR: 0.000009, Acc: 96.4527, Loss: 0.1091
LR: 0.000008, Acc: 95.7770, Loss: 0.1256
LR: 0.000008, Acc: 97.8041, Loss: 0.0820
LR: 0.000008, Acc: 96.7905, Loss: 0.1131
LR: 0.000008, Acc: 97.2973, Loss: 0.0933
LR: 0.000007, Acc: 96.6216, Loss: 0.1093
LR: 0.000007, Acc: 97.1284, Loss: 0.0887
LR: 0.000007, Acc: 98.1419, Loss: 0.0734
LR: 0.000007, Acc: 97.1284, Loss: 0.1136
LR: 0.000006, Acc: 97.9730, Loss: 0.0743
LR: 0.000006, Acc: 96.1149, Loss: 0.1397
>>> Switching to ROUTER-ONLY finetuning <<<
LR: 0.000200, Acc: 97.8041, Loss: 0.0942
LR: 0.000200, Acc: 96.4527, Loss: 0.1268
LR: 0.000199, Acc: 95.7770, Loss: 0.1185
LR: 0.000197, Acc: 97.2973, Loss: 0.0812
LR: 0.000195, Acc: 97.9730, Loss: 0.0688
LR: 0.000193, Acc: 97.4662, Loss: 0.0791
LR: 0.000190, Acc: 97.9730, Loss: 0.0733
LR: 0.000187, Acc: 97.4662, Loss: 0.0751
LR: 0.000183, Acc: 96.7905, Loss: 0.1130
LR: 0.000178, Acc: 96.9595, Loss: 0.1049
LR: 0.000173, Acc: 97.9730, Loss: 0.0940
LR: 0.000168, Acc: 97.4662, Loss: 0.0768
LR: 0.000162, Acc: 97.2973, Loss: 0.0952
LR: 0.000156, Acc: 97.9730, Loss: 0.0754
LR: 0.000150, Acc: 97.8041, Loss: 0.0776
LR: 0.000144, Acc: 97.6351, Loss: 0.0716
LR: 0.000137, Acc: 98.4797, Loss: 0.0538
LR: 0.000130, Acc: 97.1284, Loss: 0.0887
LR: 0.000123, Acc: 97.1284, Loss: 0.0962
LR: 0.000115, Acc: 96.7905, Loss: 0.1071
LR: 0.000108, Acc: 98.9865, Loss: 0.0494
LR: 0.000100, Acc: 96.4527, Loss: 0.1205
LR: 0.000093, Acc: 97.1284, Loss: 0.0922
LR: 0.000086, Acc: 96.1149, Loss: 0.1169
LR: 0.000078, Acc: 97.1284, Loss: 0.0762
LR: 0.000071, Acc: 97.1284, Loss: 0.1069
LR: 0.000064, Acc: 97.4662, Loss: 0.0903
LR: 0.000057, Acc: 96.9595, Loss: 0.1088
LR: 0.000051, Acc: 96.2838, Loss: 0.1258
LR: 0.000045, Acc: 97.6351, Loss: 0.0812
LR: 0.000039, Acc: 96.6216, Loss: 0.1293
LR: 0.000033, Acc: 97.1284, Loss: 0.0980
LR: 0.000028, Acc: 97.6351, Loss: 0.0870
LR: 0.000023, Acc: 97.2973, Loss: 0.0958
LR: 0.000018, Acc: 96.6216, Loss: 0.0987
LR: 0.000014, Acc: 97.2973, Loss: 0.0956
LR: 0.000011, Acc: 96.7905, Loss: 0.1138
LR: 0.000008, Acc: 96.2838, Loss: 0.1149
LR: 0.000006, Acc: 97.1284, Loss: 0.0880
LR: 0.000004, Acc: 96.6216, Loss: 0.0951
LR: 0.000002, Acc: 98.1419, Loss: 0.0799
LR: 0.000001, Acc: 97.9730, Loss: 0.0757
LR: 0.000001, Acc: 96.6216, Loss: 0.1070
**** Final test accuracy: 93.84. ****

LoRA weights saved to weights/vitb16/oxford_pets/16shots/2experts/seed1/CLIP-MoLE_oxford_pets.pt
